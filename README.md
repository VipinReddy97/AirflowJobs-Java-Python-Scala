# ğŸš€ Apache Airflow Supercharged with Java, Scala, and Python Spark Jobs  

This project enables **seamless orchestration of Spark jobs** written in **Python, Scala, and Java** using **Apache Airflow** in a **Dockerized environment**.  
The DAG is designed to **automate Spark job submission**, ensuring efficient and reliable **data processing pipelines** run on a scheduled basis.

---

## ğŸ—ï¸ **Project Overview**

The **`sparking_flow`** DAG executes the following tasks:

- **ğŸŸ¢ `start`** â†’ Logs `"Jobs started"` in Airflow.
- **ğŸ `python_job`** â†’ Submits a **Python Spark job**.
- **ğŸ”¥ `scala_job`** â†’ Submits a **Scala Spark job**.
- **â˜• `java_job`** â†’ Submits a **Java Spark job**.
- **âœ… `end`** â†’ Logs `"Jobs completed successfully"`.

### âœ¨ **Why is Airflow â€˜On Steroidsâ€™ Here?**
ğŸš€ **Runs Spark jobs in multiple languages** (**Python, Scala, Java**).  
ğŸ”„ **Parallel execution** of tasks to **speed up workflows**.  
ğŸ“Š **Automated scheduling, monitoring, and logging** with Airflow.  
ğŸ³ **Runs everything inside Docker**, making deployment seamless.  

---

## âš ï¸ **Why Python Works Out-of-the-Box but Java & Scala Need Setup?**
âœ… **Docker includes a Python environment by default**, so Python Spark jobs **run effortlessly**.  
âŒ **Java & Scala require compilation** before submission, which means we need:
   - **Java Development Kit (JDK)** to compile and run Java jobs.
   - **Scala Build Tool (SBT)** to package Scala jobs into `.jar` files.  
ğŸ›  **This setup ensures all Spark jobs are packaged correctly before execution.**  

---

## ğŸ“Œ **Prerequisites**
Before running this project, ensure you have the following installed:

âœ… **Docker & Docker Compose**  
âœ… **Apache Airflow Docker image** (or a custom setup)  
âœ… **Apache Spark Docker image** (or a pre-configured Spark installation)  
âœ… **Mounted Docker volumes** for Airflow DAGs, logs, and Spark jobs  

---

---

## ğŸ› ï¸ Docker Setup

To run this project using Docker, follow these steps:

1. Clone this repository to your local machine.
2. Navigate to the directory containing the `docker-compose.yml` file.
3. Build and run the containers using Docker Compose:

   ```bash
   docker-compose up -d --build
   ```

This command will start the necessary services defined in your docker-compose.yml, such as Airflow webserver, scheduler, Spark master, and worker containers
